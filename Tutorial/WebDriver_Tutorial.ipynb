{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "<div class=\"header\">\n",
    "    <div id=\"title\">\n",
    "        <center> <h1>Scraping Data in Python Using WebDrivers</h1> </center> \n",
    "    <div>\n",
    "    <div id=\"author\">\n",
    "        <center> <h3>Ryan McMahon (Penn State University)</h3> </center>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1 Introduction\n",
    " \n",
    "The Internet is a treasure trove of interesting, unique, and often underutilized data. Getting that data, however, can be an arduous task. There are numerous libraries, implemented in various programming languages, that can help to ease the burden and have been a boon to data miners everywhere. Most notable are the **_BeautifulSoup_** and **_urllib2_** libraries in Python. \n",
    "\n",
    "## 1.1 The Problem\n",
    "Still though, there are a number of instances where you may feel that these aren't the right tools for the job. Sites where you need to enter information into forms, select boxes, or navigate by dropdown menus are especially tricky when using these more traditional methods. Dynamic websites, with and without static addresses are close to impossible. \n",
    "\n",
    "## 1.2 The Solution\n",
    "WebDrivers can provide a (generally) user-friendly answer to these problems. Although this post will focus on using the **_selenium_** library paired with **_ChromeDriver_** in Python, there are other WebDrivers (e.g., **_Firefox_**, headless browsers) and languages (e.g., Java) that can be used for this. \n",
    "\n",
    "### 1.2.1 What is a WebDriver and Why is it the Solution?\n",
    "A WebDriver is simply a live instance of an Internet browser controlled by a program rather than real-time human interaction. In essence, it looks like a regular browser (both to you and sites' servers), it quacks like a regular browser, but it isn't quite a regular browser. The appeal lies in the fact that you are able to automate natural web-navigation that would be difficult or impossible with traditional *html* and *xml* extractors. \n",
    "\n",
    "A simple example is filling out a form. Say that you want to search a site for documents associated with a set of  boolean strings (e.g., [\"selenium NOT java\", \"java NOT selenium\", ...]) over a set of specific time spans. However, the address of those search results are dynamic - making them impossible to generate a priori. That's going to be a problem for other tools, but with a WebDriver you can execute the search by filling out the search bar and specifying the date range (e.g., by clicking on a calendar GUI, entering in the dates, or using a dropdown menu). \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup\n",
    "\n",
    "\n",
    "Assuming that you have Python 2.7+ up and running on your machine, the first thing that you will need is [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/). You'll want to install this somewhere that's easily accessible (I just have it in my \"Desktop\" folder). \n",
    "\n",
    "Next you will need to install **_selenium_**, which can be done via pip:\n",
    "```python\n",
    "pip install selenium\n",
    "```\n",
    "\n",
    "Optional libraries that you may find useful are: **_os_**, **_random_**, **_re_**, **_time_**, and **_sys_**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting Started\n",
    "\n",
    "\n",
    "Once those are installed, you can start getting acquainted with using a WebDriver through Python.\n",
    "\n",
    "First, we need to import the necessary libraries. The *Select* utility allows us to isolate user interfaces that we want to operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're actually going to connect to the WebDriver and open up a browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Connecting\n",
    "chromedriver = \"C:\\\\Users\\\\rbm166\\\\Desktop\\\\chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "## Opening\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "# Clear cache\n",
    "driver.delete_all_cookies()\n",
    "# Full screen\n",
    "driver.maximize_window()\n",
    "# Wait...\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open up a Google Chrome Browser page that isn't on any particular webpage:\n",
    "---\n",
    "\n",
    "<img src=\"blank_chromedriver.PNG\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Example\n",
    "\n",
    "Now that you know how to get started, let's move on to a real example.\n",
    "\n",
    "## 4.1 The *Constitute Project*\n",
    "\n",
    "The *Constitute Project* is a collection of national constitutions from across the world. Users can read full constitutions, compare countries, and explore topics within and across documents. This database, however, would be very difficult to scrape without using a WebDriver. This is because the site's main page listing the constitutions is \n",
    "dynamic and based on JavaScript. \n",
    "\n",
    "Using Python and ChromeDriver, we can easily navigate this page and grab all the links to countries constitutions, whose pages are static HTML and can be parsed with a standard library like **_BeautifulSoup_**. \n",
    "\n",
    "First, we're going to point our driver to the page we want to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Provide the starting page:\n",
    "link0 = 'https://www.constituteproject.org/search?lang=en'\n",
    "\n",
    "## Go to that starting page:\n",
    "driver.get(link0)\n",
    "\n",
    "## Wait for it to load:\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, the driver's browser should look something like this:\n",
    "<img src=\"constituteproject_searchpage.PNG\">\n",
    "\n",
    "\n",
    "Using the inspect element option available in that window or a separate browser, we can find the identifier associated with the links that we want to grab. In this case, we can use the link text \"**View HTML**\".\n",
    "\n",
    "Now we can tell our driver to find all of the page's elements associated with that link text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pull all \"View HTML\" related objects:\n",
    "objects = driver.find_elements_by_link_text('View HTML')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a quick check to make sure that we're only grabbing what we want. The number of objects should equal the number of constitutions, *n = 194*, listed at the top of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity Check:\n",
    "len(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to get the links out of these objects now. **_Selenium_** makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'https://www.constituteproject.org/constitution/Afghanistan_2004?lang=en',\n",
       " u'https://www.constituteproject.org/constitution/Albania_2012?lang=en',\n",
       " u'https://www.constituteproject.org/constitution/Algeria_2008?lang=en',\n",
       " u'https://www.constituteproject.org/constitution/Andorra_1993?lang=en',\n",
       " u'https://www.constituteproject.org/constitution/Angola_2010?lang=en',\n",
       " u'https://www.constituteproject.org/constitution/Antigua_and_Barbuda_1981?lang=en']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## List to hold the links:\n",
    "links = []\n",
    "\n",
    "## Iterate over list and get the links:\n",
    "for obj in objects:\n",
    "    links.append(obj.get_attribute('href'))\n",
    "\n",
    "## Inspect the links:\n",
    "links[0:6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These links can now be written out to a **\\*.txt** or **\\*.csv** file and parsed using a library like **_BeautifulSoup_**, or manipulated later in the script. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now we can close the driver:\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Traditional web scraping libraries and packages are well developed tools that make web scraping easier. They do, however, fall short on some fronts. WebDrivers provide an elegant solution to many of the problems faced by these traditional methods. As shown above, WebDrivers can navigate dynamic websites with ease and are easily adaptable to most situations. \n",
    "\n",
    "\n",
    "### Disclaimer\n",
    "Just because you can doesn't mean you should. Be sure to check and observe sites' policies regarding scrapers. These policies can most often be found at \"*[insert url here].com**/robots.txt***\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
